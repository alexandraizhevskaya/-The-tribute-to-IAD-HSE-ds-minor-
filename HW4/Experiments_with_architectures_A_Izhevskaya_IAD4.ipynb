{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Experiments_with_architectures.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OahDZ2tzuly4",
        "colab_type": "text"
      },
      "source": [
        "В данном ноутбуке рассмотрены несколько современных моделей, основанных на базовой архитектуре \"Трансформер\". BERT (или Bidirectional Encoder Representations from Transformers) - модель, которая позволяет дофайнтьюнить заранее обученные при помощи MLM эмбеддинги. На данный момент существует много вариаций данной модели, включая многоязычные.   Здесь будут рассмотрены несколько современнных архитектур, применяемых для решения сложных задач в сфере нлп."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8wV1CKouly7",
        "colab_type": "text"
      },
      "source": [
        "Прежде чем переходить к основной части задачи необходимо, собственно, более подробно описать ее. Jigsaw Multilingual Toxic Comment Classification - это задача по классификации комментариев на два вида: токсичные и нетоксичные. Фактически, эта задача представляет собой бинарную классификацию на 2 класса: токсичные (1) и нетоксичные (0). Таким образом, мы предсказываем вероятность того, что комментарий окажется токсичным. Особенностью данного соревнования является то, что в тренировочной выборке присутствуют только англоязычные комментарии, а вот в валидационном и треничовочном датасетах есть комментарии на разных языках. Теперь приступим к обучению самих моделей.\n",
        "\n",
        "Из-за особенностей kaggle-only соревнований, с помощью одного ноутбука можно получить только один сабмит, поэтому здесь представлены только результаты обучения разных моделей: финальный сабмит был получен с помощью другого ноутбука, который также приложен к этому дз.  Непосредственно на кэггл данный ноутбук запусался 3 раза, чтобы сохранить предсказание каждой модели отдельно (код остальных был закомментирован) и засабмитить его"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8a9vyRpuly8",
        "colab_type": "text"
      },
      "source": [
        "# Модель 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njEzhG_Wuly-",
        "colab_type": "text"
      },
      "source": [
        "## xlm roberta large aka самая лучшая модель"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJqYqEkPuly_",
        "colab_type": "text"
      },
      "source": [
        "Было изучено много ноутбуков - самую высокую точность демонстрировала именно данная модель или же ансамбль на ее базе - она является самой современной (СОТА) для решения подобных задач. Мой лучший сабмит также был получен с помощью данной модели. Одна обученная модель xlm roberta large c правильно подобранными параметрами позволяет уже достичь качества приблизительно 0.936. Мой самый лучший сабмит был получен с помощью взятия среднего 4 моделей данного типа с разными параметрами и 2 разными видами обработки (базовая и более тщательная с удалением всех ненужных символов, которые по идее превносят шум)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4hwMxjSulzA",
        "colab_type": "text"
      },
      "source": [
        "Тут представлен, собственно, достаточно базовый пайплайн обработки данных и обучения модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO0D05xzulzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from kaggle_datasets import KaggleDatasets\n",
        "import transformers\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "from tqdm.notebook import tqdm\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6ToNB0YulzJ",
        "colab_type": "text"
      },
      "source": [
        "Функция для предобработки данных и приведения их к формату, который использует BERT. Здесь текст каждого комментария кодируется специальным токенайзером, который возвращает уже числовые значения, также для токенайзера устанавливается определенная максимальная длина и, чтобы запихнуть разные по длине предложения в батчи одного размера, добавляется паддинг."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "PxElId6wulzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def regular_encode(texts, tokenizer, maxlen=512):\n",
        "    enc_di = tokenizer.batch_encode_plus(\n",
        "        texts, \n",
        "        return_attention_masks=False, \n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=True,\n",
        "        max_length=maxlen\n",
        "    )\n",
        "    \n",
        "    return np.array(enc_di['input_ids'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VktMZpWFulzV",
        "colab_type": "text"
      },
      "source": [
        "А эта функция создает саму модель. Первый слой - инпут с шейп, равной максимальной длине последовательности, затем слой с самой моделью, возвращающий hidden_state трансформера, что и будет являться нашим эмбеддингом, ну а затем прогоняем все через полносвязный слой с выходной размерностью, равной количеству классов (2 в данном случае), который уже и выдает нужную вероятность того, что комментарий toxic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIvsFv_dulzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(transformer, max_len=512, lr=1e-5, loss='binary_crossentropy'):\n",
        "    \"\"\"\n",
        "    https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\n",
        "    \"\"\"\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    sequence_output = transformer(input_word_ids)[0]\n",
        "    cls_token = sequence_output[:, 0, :]\n",
        "    out = Dense(1, activation='sigmoid')(cls_token)\n",
        "    \n",
        "    model = Model(inputs=input_word_ids, outputs=out)\n",
        "    model.compile(Adam(lr=lr), loss=loss, metrics=['accuracy', tf.keras.metrics.AUC()])\n",
        "    \n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXae7jKvulzd",
        "colab_type": "text"
      },
      "source": [
        "Настраиваем TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYkeaqjnulze",
        "colab_type": "code",
        "outputId": "60aa57d3-3e40-4c8e-f56c-572465a46a83",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on TPU  grpc://10.0.0.2:8470\n",
            "REPLICAS:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPeGSyTgul0J",
        "colab_type": "text"
      },
      "source": [
        "Блок гиперпараметров, которые можно варьировать для тюнинга"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Mq5JvMMul0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "EPOCHS = 2\n",
        "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "MAX_LEN = 192\n",
        "MODEL = 'jplu/tf-xlm-roberta-large'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36WRR4i7ul0a",
        "colab_type": "text"
      },
      "source": [
        "Теперь, когда функции определены, можно приступать непосредственно к подготовке данных и обучению."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8U_PCl8ul0b",
        "colab_type": "code",
        "outputId": "fa5892fb-67a0-431f-c403-324901e79567",
        "colab": {
          "referenced_widgets": [
            "0c30c5d1031a421b83aa6ee761b86d71",
            "edaa96b456034cc2835ef7356137c678"
          ]
        }
      },
      "source": [
        "# сам токенайзер\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c30c5d1031a421b83aa6ee761b86d71",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=513.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "edaa96b456034cc2835ef7356137c678",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQwvQdxYul0h",
        "colab_type": "text"
      },
      "source": [
        "Загружаем данные из кэггл"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2rME8M2ul0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train1 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n",
        "train2 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\")\n",
        "train2.toxic = train2.toxic.round().astype(int)\n",
        "\n",
        "valid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\n",
        "test = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')\n",
        "sub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')\n",
        "\n",
        "# добавляем подмножество из втрого тренировочного датасета в 1\n",
        "train = pd.concat([\n",
        "    train1[['comment_text', 'toxic']],\n",
        "    train2[['comment_text', 'toxic']].query('toxic==1'),\n",
        "    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=100000, random_state=0)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4AktziUul0o",
        "colab_type": "text"
      },
      "source": [
        "Применяем функцию предобработки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amRV3vkQul0p",
        "colab_type": "code",
        "outputId": "802d5db6-410b-421c-f4ca-081e3233ee22",
        "colab": {}
      },
      "source": [
        "%%time \n",
        "\n",
        "x_train = regular_encode(train.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
        "x_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
        "x_test = regular_encode(test.content.values, tokenizer, maxlen=MAX_LEN)\n",
        "\n",
        "y_train = train.toxic.values\n",
        "y_valid = valid.toxic.values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 7min 28s, sys: 2.01 s, total: 7min 30s\n",
            "Wall time: 7min 30s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkSKeahIul0u",
        "colab_type": "text"
      },
      "source": [
        "Оборачиваем обычные эррэи в тензорфлоу датасет, чтобы удобно работать с батчами"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vxnXCZgul0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_train, y_train))\n",
        "    .repeat()\n",
        "    .shuffle(2048)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "valid_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_valid, y_valid))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "test_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices(x_test)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x-kO7ZIul01",
        "colab_type": "text"
      },
      "source": [
        "Создаем саму модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od-WxB2Uul02",
        "colab_type": "code",
        "outputId": "08d94560-52f8-4cec-b107-556376d2088b",
        "colab": {
          "referenced_widgets": [
            "06eb67e3194d4c4e93d7eb37085fa0cd"
          ]
        }
      },
      "source": [
        "%%time\n",
        "with strategy.scope():\n",
        "    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
        "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06eb67e3194d4c4e93d7eb37085fa0cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=3271420488.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_word_ids (InputLayer)  [(None, 192)]             0         \n",
            "_________________________________________________________________\n",
            "tf_roberta_model (TFRobertaM ((None, 192, 1024), (None 559890432 \n",
            "_________________________________________________________________\n",
            "tf_op_layer_strided_slice (T [(None, 1024)]            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 1025      \n",
            "=================================================================\n",
            "Total params: 559,891,457\n",
            "Trainable params: 559,891,457\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "CPU times: user 2min 3s, sys: 38.9 s, total: 2min 42s\n",
            "Wall time: 2min 41s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smq6Wmurul08",
        "colab_type": "text"
      },
      "source": [
        "Дальше идет обучение. во всех изученных мной ноутбуках схема была примерно такая: 2-4 эпохи обучения на обучающей выборке, еще 2-4 на валидационной. Здесь также - сначала обучение на трейн:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4D82cr-ul08",
        "colab_type": "code",
        "outputId": "afd1ebe3-84a4-4e92-f364-e91568f9ccfe",
        "colab": {}
      },
      "source": [
        "n_steps = x_train.shape[0] // BATCH_SIZE\n",
        "train_history = model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=n_steps,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=EPOCHS\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 3404 steps, validate for 63 steps\n",
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:430: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
            "  num_elements)\n",
            "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:430: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
            "  num_elements)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3404/3404 [==============================] - 1839s 540ms/step - loss: 0.0696 - accuracy: 0.9722 - auc: 0.9956 - val_loss: 0.4936 - val_accuracy: 0.8463 - val_auc: 0.4896\n",
            "Epoch 2/2\n",
            "3404/3404 [==============================] - 1631s 479ms/step - loss: 0.0664 - accuracy: 0.9752 - auc: 0.9959 - val_loss: 0.2827 - val_accuracy: 0.8714 - val_auc: 0.9062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np_lWk03ul1D",
        "colab_type": "text"
      },
      "source": [
        "А теперь еще несколько эпох на валидационной части"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khw1Ja5Pul1H",
        "colab_type": "code",
        "outputId": "53390e51-6ac4-4131-d206-6fab3f84c71c",
        "colab": {}
      },
      "source": [
        "n_steps = x_valid.shape[0] // BATCH_SIZE\n",
        "train_history_2 = model.fit(\n",
        "    valid_dataset.repeat(),\n",
        "    steps_per_epoch=n_steps,\n",
        "    epochs=EPOCHS\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 62 steps\n",
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:430: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
            "  num_elements)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "62/62 [==============================] - 122s 2s/step - loss: 0.2476 - accuracy: 0.8881 - auc: 0.9191\n",
            "Epoch 2/2\n",
            "62/62 [==============================] - 140s 2s/step - loss: 0.1807 - accuracy: 0.9196 - auc: 0.9583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5W3Lx-gul1L",
        "colab_type": "text"
      },
      "source": [
        "Сабмит одной модели xlm roberta large готов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKlIMT25ul1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sub['toxic'] = model.predict(test_dataset, verbose=1)\n",
        "# sub.to_csv('submission.csv', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR7KadavxcCE",
        "colab_type": "text"
      },
      "source": [
        "А вот и результат, который достаточно высок \n",
        "![alt text](https://sun9-51.userapi.com/c813024/v813024533/d78f8/q94JZBD5HEk.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Dkq7voMul1R",
        "colab_type": "text"
      },
      "source": [
        "## модель 2 distilbert "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8Zi3KBzul1S",
        "colab_type": "text"
      },
      "source": [
        "Пайплайн здесь точно такой же, мы просто заменяем \"сердце\" нашей модели (трансформер) на distilbert. Обрабатываем данные нужным токенайзером, оборачиваем в датасет, создаем новую модель с такой же архитектурой, но с distilbert внутри. К сожалению, нет варианта bert (или distilbert) c hidden layer 1024: модель слабее (768). Обучаем на трейне и валидации - в данном случае 4 эпохи. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI5ECRPsul1U",
        "colab_type": "code",
        "outputId": "b138960c-1976-4fba-a754-e0f9a4079d69",
        "colab": {
          "referenced_widgets": [
            "14b764965b6944dbbf6de4b6935e69fa",
            "9ed23c8091854311ac868d750c0951c5"
          ]
        }
      },
      "source": [
        "MODEL = 'distilbert-base-multilingual-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14b764965b6944dbbf6de4b6935e69fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ed23c8091854311ac868d750c0951c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfeHwg9Uul1d",
        "colab_type": "text"
      },
      "source": [
        "создаем датасет"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A5uVKGFul1e",
        "colab_type": "code",
        "outputId": "ac1e17ff-d913-402a-963f-47030fe2d7dd",
        "colab": {}
      },
      "source": [
        "%%time \n",
        "\n",
        "x_train = regular_encode(train.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
        "x_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
        "x_test = regular_encode(test.content.values, tokenizer, maxlen=MAX_LEN)\n",
        "\n",
        "y_train = train.toxic.values\n",
        "y_valid = valid.toxic.values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 19min 31s, sys: 1.01 s, total: 19min 32s\n",
            "Wall time: 19min 31s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmLpVGKOul1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_train, y_train))\n",
        "    .repeat()\n",
        "    .shuffle(2048)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "valid_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_valid, y_valid))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "test_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices(x_test)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L101wonuul1j",
        "colab_type": "text"
      },
      "source": [
        "Создаем модель и обучаем"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXlekeFAul1j",
        "colab_type": "code",
        "outputId": "d416e9bd-5370-4706-e89d-abde2b86d787",
        "colab": {
          "referenced_widgets": [
            "d7431ba050034bf9875873adc735502c"
          ]
        }
      },
      "source": [
        "%%time\n",
        "with strategy.scope():\n",
        "    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
        "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7431ba050034bf9875873adc735502c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=910749124.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_word_ids (InputLayer)  [(None, 192)]             0         \n",
            "_________________________________________________________________\n",
            "tf_distil_bert_model (TFDist ((None, 192, 768),)       134734080 \n",
            "_________________________________________________________________\n",
            "tf_op_layer_strided_slice_1  [(None, 768)]             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 769       \n",
            "=================================================================\n",
            "Total params: 134,734,849\n",
            "Trainable params: 134,734,849\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "CPU times: user 33.7 s, sys: 10.1 s, total: 43.8 s\n",
            "Wall time: 47.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CV_eZequl1m",
        "colab_type": "code",
        "outputId": "8d125b1d-02f5-494c-9265-c5ad6332c1ef",
        "colab": {}
      },
      "source": [
        "n_steps = x_train.shape[0] // BATCH_SIZE\n",
        "train_history = model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=n_steps,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=EPOCHS*2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 3404 steps, validate for 63 steps\n",
            "Epoch 1/4\n",
            "3404/3404 [==============================] - 432s 127ms/step - loss: 0.0795 - accuracy: 0.9696 - auc_1: 0.9941 - val_loss: 0.3943 - val_accuracy: 0.8446 - val_auc_1: 0.7907\n",
            "Epoch 2/4\n",
            "3404/3404 [==============================] - 375s 110ms/step - loss: 0.0590 - accuracy: 0.9768 - auc_1: 0.9968 - val_loss: 0.4962 - val_accuracy: 0.8469 - val_auc_1: 0.8190\n",
            "Epoch 3/4\n",
            "3404/3404 [==============================] - 375s 110ms/step - loss: 0.0510 - accuracy: 0.9797 - auc_1: 0.9976 - val_loss: 0.6480 - val_accuracy: 0.8469 - val_auc_1: 0.7588\n",
            "Epoch 4/4\n",
            "1982/3404 [================>.............] - ETA: 2:36 - loss: 0.0655 - accuracy: 0.9731 - auc_1: 0.9953"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UGnhYrIul1o",
        "colab_type": "code",
        "outputId": "356d1afc-e2d3-4e43-d1d1-9e43ac017960",
        "colab": {}
      },
      "source": [
        "n_steps = x_valid.shape[0] // BATCH_SIZE\n",
        "train_history_2 = model.fit(\n",
        "    valid_dataset.repeat(),\n",
        "    steps_per_epoch=n_steps,\n",
        "    epochs=EPOCHS*2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 62 steps\n",
            "Epoch 1/4\n",
            "62/62 [==============================] - 30s 487ms/step - loss: 0.3754 - accuracy: 0.8506 - auc_1: 0.7757\n",
            "Epoch 2/4\n",
            "62/62 [==============================] - 24s 393ms/step - loss: 0.2429 - accuracy: 0.8923 - auc_1: 0.9187\n",
            "Epoch 3/4\n",
            "62/62 [==============================] - 7s 108ms/step - loss: 0.1788 - accuracy: 0.9201 - auc_1: 0.9592\n",
            "Epoch 4/4\n",
            "62/62 [==============================] - 7s 112ms/step - loss: 0.1152 - accuracy: 0.9508 - auc_1: 0.9839\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psAkyM7nul1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sub['toxic'] = model.predict(test_dataset, verbose=1)\n",
        "# sub.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JpNmsXpyIdX",
        "colab_type": "text"
      },
      "source": [
        "Несмотря на высокие показатели при мониторинге на трейн и тест, на паблик качество хуже:\n",
        "\n",
        "![alt text](https://sun9-53.userapi.com/c813024/v813024533/d791c/EVQz1sZe_tM.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS-hr9Z5ul1v",
        "colab_type": "text"
      },
      "source": [
        "## модель 3 xlm \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usj5W1qwul1w",
        "colab_type": "text"
      },
      "source": [
        "К сожалению, погуглив multilingual models, я поняла, что их выбор не особо широк: hugging face предложили, собственно, Берт ( distilbert - его вариация соответственно), XLM-Роберта и XLM, но последнюю модель я не встречала ни в одном из кернелов, так что результаты непредсказуемы\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGCu_rt-ul1w",
        "colab_type": "code",
        "outputId": "4da51019-83f6-471d-b00a-779948d7c43f",
        "colab": {
          "referenced_widgets": [
            "1020f4e5df414de497aa18e45e1fb802",
            "b9920939a11d42118fb78fddee6a80ac",
            "a74a55c92fb246b38badeb5bb262a619"
          ]
        }
      },
      "source": [
        "MODEL = 'xlm-mlm-100-1280'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1020f4e5df414de497aa18e45e1fb802",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1399.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9920939a11d42118fb78fddee6a80ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2952532.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a74a55c92fb246b38badeb5bb262a619",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1434601.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-9Y53I0ul10",
        "colab_type": "code",
        "outputId": "531ab084-c454-4cee-dbee-35abf4b191b9",
        "colab": {}
      },
      "source": [
        "%%time \n",
        "\n",
        "x_train = regular_encode(train.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
        "x_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
        "x_test = regular_encode(test.content.values, tokenizer, maxlen=MAX_LEN)\n",
        "\n",
        "y_train = train.toxic.values\n",
        "y_valid = valid.toxic.values\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 21min 16s, sys: 3.48 s, total: 21min 19s\n",
            "Wall time: 21min 19s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nkx0QtTaul13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_train, y_train))\n",
        "    .repeat()\n",
        "    .shuffle(2048)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "valid_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_valid, y_valid))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "test_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices(x_test)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci4psE106Dl-",
        "colab_type": "text"
      },
      "source": [
        "Создание модели и  процесс обучения. Так как нельзя запустить все 3 модели в одном кернеле, чтобы сразу сохранить все аутпуты (Resources Exhausted error) - эти части закомментированы, прикладываю скрины \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io1KYjo3ul15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%time\n",
        "# with strategy.scope():\n",
        "#     transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
        "#     model = build_model(transformer_layer, max_len=MAX_LEN)\n",
        "# model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRkg18CUul18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# n_steps = x_train.shape[0] // BATCH_SIZE\n",
        "# train_history = model.fit(\n",
        "#     train_dataset,\n",
        "#     steps_per_epoch=n_steps,\n",
        "    \n",
        "#     validation_data=valid_dataset,\n",
        "#     epochs=EPOCHS\n",
        "# )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKcrSyjGul1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# n_steps = x_valid.shape[0] // BATCH_SIZE\n",
        "# train_history_2 = model.fit(\n",
        "#     valid_dataset.repeat(),\n",
        "#     steps_per_epoch=n_steps,\n",
        "#     epochs=EPOCHS\n",
        "# )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwTOBy5Cul2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sub['toxic'] = model.predict(test_dataset, verbose=1)\n",
        "# sub.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiZx69dg7t_t",
        "colab_type": "text"
      },
      "source": [
        "Архитектура модели:\n",
        "\n",
        "![alt text](https://sun9-4.userapi.com/c857528/v857528479/205d53/BBgTF3jllYw.jpg)\n",
        "\n",
        "Процесс обучения на трейн:\n",
        "\n",
        "![alt text](https://sun9-41.userapi.com/c857528/v857528479/205d65/YOqtQz7ys0w.jpg)\n",
        "\n",
        "Процесс обучения на валид:\n",
        "\n",
        "![alt text](https://sun9-68.userapi.com/c857528/v857528479/205d5c/God1-ks_caU.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2XBoU5v569M",
        "colab_type": "text"
      },
      "source": [
        "Качество на паблик в итоге лучше, чем у distilbert, но сильно уступает Robertta:\n",
        "![alt text](https://sun9-28.userapi.com/c857528/v857528479/205d6e/GxTfDAYXcG4.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRW-PqYF96e3",
        "colab_type": "text"
      },
      "source": [
        "# Краткие выводы, касательно данной задачи и моделей, примененных для ее решения\n",
        "\n",
        "Главной особенностью задачи является то, что в множестве, на котором происходит обучение, состоит из комментариев на английском языке, а вот валидационный и тестовый - включает в себя много разных языков:\n",
        "Это существенно затрудняет задачу, так как модель, обученная на одном домене (языке), по сути, должна решать задачу для других доменов. Это значит, что, несмотря на разнообразие современны моделей трансформеров, их выбор снижается лишь до малтилингуал (всего 3 модели, указаны в списке hugging face), так как остальные обучаются на конкретном языке и хорошо работают лишь на нем. Берт в данном случае является хорошей базовой моделью, но, тем не менее, он не совершенен: cross lingual models (xlm) - следующий шаг для решения данной задачи, в который добавлены некоторые улучшения, помогающие достичь лучшего качества. В данной задаче distilbert  оказался хорошим бейзлайном: он показал неплохое, но далеко не эталонное качество, xlm оказался лучше, ну а самый лучший же результат продемонстрировала Роберта.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmXkjGZIAY_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = pd.DataFrame(list(zip(['xlm-roberta-large', 'xlm-mlm-100-1280', 'distilbert-base-multilingual-cased'], [0.9362, 0.8884, 0.8718])), columns=['model', 'test_auc_public'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GurA2avJGltB",
        "colab_type": "code",
        "outputId": "abcb5a45-d63f-4379-ee9d-63f9acc2f9a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "res"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>test_auc_public</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.9362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>xlm-mlm-100-1280</td>\n",
              "      <td>0.8884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>distilbert-base-multilingual-cased</td>\n",
              "      <td>0.8718</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                model  test_auc_public\n",
              "0                   xlm-roberta-large           0.9362\n",
              "1                    xlm-mlm-100-1280           0.8884\n",
              "2  distilbert-base-multilingual-cased           0.8718"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alBy2FsSIaln",
        "colab_type": "text"
      },
      "source": [
        " References:\n",
        "\n",
        "Все открытые ноутбуки на кэггл по соревнованию. Большинство из них очень сильно похожи и повторяют тот же самый пайплайн, примененный здесь. Было просмотрено большое количество кернелов и основа была взята из них."
      ]
    }
  ]
}