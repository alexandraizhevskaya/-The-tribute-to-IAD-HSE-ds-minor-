{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Main_model_with_ensembling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2yyI5JENA8U",
        "colab_type": "text"
      },
      "source": [
        "Это второй ноутбук по соревнованию. Здесь показано, как было получено лучшее предсказание. Из ноутбука со сравнением было видно, что модель XLM-Роберта оказалась лучшей для данной задачи, она одна уже пробивала проходной порог для домашней работы (0.9), однако, так как попадалось огромное количество ноутбуков с ансамблями, я решила также попробовать объединить предсказания нескольких моделей, основанных на лучшей модели и улучшить предсказание до 0.94. В данном ноутбуке показано как были обучены модели. Из-за особенностей kaggle kernel-only, этот ноутбук сначала запускался 3 раза, чтобы сохранить 3 разных предсказания. Затем уже сохраненные предсказания подгружались в кернел как сторонний инпут, чтобы объединить их с весами, и ноутбук запускался 4 раз, чтобы получить финальный сабмит как аутпут этого ноутбука и соответственно засабмитить его."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2YlNnwPRfKw",
        "colab_type": "text"
      },
      "source": [
        "Данная часть полностью совпадает с той, что была приведена в ноутбуке со сравнением"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "SCiPjFi9L09N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras import backend as K\n",
        "from kaggle_datasets import KaggleDatasets\n",
        "import transformers\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "from tqdm.notebook import tqdm\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzwls0Q4UoSN",
        "colab_type": "text"
      },
      "source": [
        " Функция для предобработки для трансформера"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3jXhEc6FL09x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def regular_encode(texts, tokenizer, maxlen=512):\n",
        "    enc_di = tokenizer.batch_encode_plus(\n",
        "        texts, \n",
        "        return_attention_masks=False, \n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=True,\n",
        "        max_length=maxlen\n",
        "    )\n",
        "    \n",
        "    return np.array(enc_di['input_ids'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM3FVNfeUy0D",
        "colab_type": "text"
      },
      "source": [
        "Функция для создания модели "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7jazikvuL0-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(transformer, max_len=512, lr=1e-5, loss='binary_crossentropy'):\n",
        "    \"\"\"\n",
        "    https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\n",
        "    \"\"\"\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    sequence_output = transformer(input_word_ids)[0]\n",
        "    cls_token = sequence_output[:, 0, :]\n",
        "    out = Dense(1, activation='sigmoid')(cls_token)\n",
        "    \n",
        "    model = Model(inputs=input_word_ids, outputs=out)\n",
        "    model.compile(Adam(lr=lr), loss=loss, metrics=['accuracy', tf.keras.metrics.AUC()])\n",
        "    \n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah4x9jZHVgrA",
        "colab_type": "text"
      },
      "source": [
        "Настройка ТПУ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "E2fzPzSeL0-m",
        "colab_type": "code",
        "outputId": "bc23e866-53a2-4511-dd23-3e3603814a1e",
        "colab": {}
      },
      "source": [
        "\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on TPU  grpc://10.0.0.2:8470\n",
            "REPLICAS:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0EREipaVxZc",
        "colab_type": "text"
      },
      "source": [
        "Гиперпараметры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tNHgFIb-L0_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "EPOCHS = 2\n",
        "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "MAX_LEN = 192\n",
        "MODEL = 'jplu/tf-xlm-roberta-large'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1cqlB0GGL0_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# сам токенайзер\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zukAdcQV1n8",
        "colab_type": "text"
      },
      "source": [
        "Предобработка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5CKuKTNVL1AB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train1 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n",
        "train2 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\")\n",
        "train2.toxic = train2.toxic.round().astype(int)\n",
        "\n",
        "valid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\n",
        "test = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')\n",
        "sub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cyHeEVICL1AW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.concat([\n",
        "    train1[['comment_text', 'toxic']],\n",
        "    train2[['comment_text', 'toxic']].query('toxic==1'),\n",
        "    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=100000, random_state=0)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3Pu2684IL1BF",
        "colab_type": "code",
        "outputId": "01df7d48-235f-436e-84bf-9db90cbc3b77",
        "colab": {}
      },
      "source": [
        "%%time \n",
        "\n",
        "x_train = regular_encode(train.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
        "x_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
        "x_test = regular_encode(test.content.values, tokenizer, maxlen=MAX_LEN)\n",
        "\n",
        "y_train = train.toxic.values\n",
        "y_valid = valid.toxic.values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 7min 5s, sys: 1.86 s, total: 7min 7s\n",
            "Wall time: 7min 6s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElN1xcAgV4Vn",
        "colab_type": "text"
      },
      "source": [
        "Создание датасета и обучение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7Yg2i4FXL1Bf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_train, y_train))\n",
        "    .repeat()\n",
        "    .shuffle(2048)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "valid_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_valid, y_valid))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "test_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices(x_test)\n",
        "    .batch(BATCH_SIZE)\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2mnaaG99L1B2",
        "colab_type": "code",
        "outputId": "a1dd5f90-fee2-4357-a414-99488fd7ddbe",
        "colab": {
          "referenced_widgets": [
            "0609de3cf61b4c24b05747eecaf65a6f"
          ]
        }
      },
      "source": [
        "%%time\n",
        "with strategy.scope():\n",
        "    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
        "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=3271420488.0, style=ProgressStyle(descr…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0609de3cf61b4c24b05747eecaf65a6f"
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_word_ids (InputLayer)  [(None, 192)]             0         \n",
            "_________________________________________________________________\n",
            "tf_roberta_model (TFRobertaM ((None, 192, 1024), (None 559890432 \n",
            "_________________________________________________________________\n",
            "tf_op_layer_strided_slice (T [(None, 1024)]            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 1025      \n",
            "=================================================================\n",
            "Total params: 559,891,457\n",
            "Trainable params: 559,891,457\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "CPU times: user 2min 1s, sys: 39.9 s, total: 2min 41s\n",
            "Wall time: 2min 40s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AoutJnhDL1D_",
        "colab_type": "code",
        "outputId": "e0e3b405-39e7-4ca9-eb40-98a33b725c6b",
        "colab": {}
      },
      "source": [
        "n_steps = x_train.shape[0] // BATCH_SIZE\n",
        "train_history = model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=n_steps,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=EPOCHS\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 3404 steps, validate for 63 steps\n",
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:430: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
            "  num_elements)\n",
            "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:430: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
            "  num_elements)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3404/3404 [==============================] - 1840s 540ms/step - loss: 0.0719 - accuracy: 0.9725 - auc: 0.9950 - val_loss: 0.2925 - val_accuracy: 0.8650 - val_auc: 0.9006\n",
            "Epoch 2/2\n",
            "3404/3404 [==============================] - 1633s 480ms/step - loss: 0.0510 - accuracy: 0.9794 - auc: 0.9976 - val_loss: 0.3137 - val_accuracy: 0.8648 - val_auc: 0.9029\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aEHstA8VL1ED",
        "colab_type": "code",
        "outputId": "048a1e07-72b0-486b-f17a-1e33375fde84",
        "colab": {}
      },
      "source": [
        "n_steps = x_valid.shape[0] // BATCH_SIZE\n",
        "train_history_2 = model.fit(\n",
        "    valid_dataset.repeat(),\n",
        "    steps_per_epoch=n_steps,\n",
        "    epochs=EPOCHS\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 62 steps\n",
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:430: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
            "  num_elements)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "62/62 [==============================] - 123s 2s/step - loss: 0.2312 - accuracy: 0.8925 - auc: 0.9289\n",
            "Epoch 2/2\n",
            "62/62 [==============================] - 141s 2s/step - loss: 0.1468 - accuracy: 0.9360 - auc: 0.9731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRoaslADV7Vr",
        "colab_type": "text"
      },
      "source": [
        "Сохранение предсказания"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ng4nplwJL1EK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sub['toxic'] = model.predict(test_dataset, verbose=1)\n",
        "# sub.to_csv('submission_4.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su4v68IbWWI_",
        "colab_type": "text"
      },
      "source": [
        "Теперь еще немного дообучаем эту модель и сохраняем второе предсказание"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "dMbfpXo_L1ER",
        "colab_type": "code",
        "outputId": "1f517c49-a99e-40b2-f0e0-8f12949eb813",
        "colab": {}
      },
      "source": [
        "n_steps = x_valid.shape[0] // BATCH_SIZE\n",
        "train_history_2 = model.fit(\n",
        "    valid_dataset.repeat(),\n",
        "    steps_per_epoch=n_steps,\n",
        "    epochs=EPOCHS\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 62 steps\n",
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:430: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
            "  num_elements)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "62/62 [==============================] - 124s 2s/step - loss: 0.0985 - accuracy: 0.9590 - auc: 0.9870\n",
            "Epoch 2/2\n",
            "62/62 [==============================] - 115s 2s/step - loss: 0.0613 - accuracy: 0.9782 - auc: 0.9951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Uq0GOQ4QL1EV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sub['toxic'] = model.predict(test_dataset, verbose=1)\n",
        "# sub.to_csv('submission_6.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M17QQVXkWdfo",
        "colab_type": "text"
      },
      "source": [
        "Теперь добавляем новый элемент - обучение той же модели, но с добавлением некоторых особенностей обучения: другой лернинг рейт+ шедулер и дополнительная предобработка данных до использования токенайзера"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uNYBXZTjL1Ea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = test\n",
        "train_data = train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM8pQXkZbdNh",
        "colab_type": "text"
      },
      "source": [
        "Убираем мусор с помощью регулярных выражений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4fsOjJKvL1Ei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val = valid\n",
        "train = train_data\n",
        "\n",
        "def clean(text):\n",
        "    text = text.fillna(\"fillna\").str.lower()\n",
        "    text = text.map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
        "    text = text.map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n",
        "    text = text.map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n",
        "    text = text.map(lambda x: re.sub(\"\\(http://.*?\\s\\(http://.*\\)\",'',str(x)))\n",
        "    return text\n",
        "\n",
        "val[\"comment_text\"] = clean(val[\"comment_text\"])\n",
        "test_data[\"content\"] = clean(test_data[\"content\"])\n",
        "train[\"comment_text\"] = clean(train[\"comment_text\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxA-PpEjbida",
        "colab_type": "text"
      },
      "source": [
        "А также избавляемся от символов и сокращений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LFpAbqlqL1Ep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\xa0', '\\t',\n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
        "\n",
        "mispell_dict = {\"aren't\" : \"are not\",\n",
        "\"can't\" : \"cannot\",\n",
        "\"couldn't\" : \"could not\",\n",
        "\"couldnt\" : \"could not\",\n",
        "\"didn't\" : \"did not\",\n",
        "\"doesn't\" : \"does not\",\n",
        "\"doesnt\" : \"does not\",\n",
        "\"don't\" : \"do not\",\n",
        "\"hadn't\" : \"had not\",\n",
        "\"hasn't\" : \"has not\",\n",
        "\"haven't\" : \"have not\",\n",
        "\"havent\" : \"have not\",\n",
        "\"he'd\" : \"he would\",\n",
        "\"he'll\" : \"he will\",\n",
        "\"he's\" : \"he is\",\n",
        "\"i'd\" : \"I would\",\n",
        "\"i'd\" : \"I had\",\n",
        "\"i'll\" : \"I will\",\n",
        "\"i'm\" : \"I am\",\n",
        "\"isn't\" : \"is not\",\n",
        "\"it's\" : \"it is\",\n",
        "\"it'll\":\"it will\",\n",
        "\"i've\" : \"I have\",\n",
        "\"let's\" : \"let us\",\n",
        "\"mightn't\" : \"might not\",\n",
        "\"mustn't\" : \"must not\",\n",
        "\"shan't\" : \"shall not\",\n",
        "\"she'd\" : \"she would\",\n",
        "\"she'll\" : \"she will\",\n",
        "\"she's\" : \"she is\",\n",
        "\"shouldn't\" : \"should not\",\n",
        "\"shouldnt\" : \"should not\",\n",
        "\"that's\" : \"that is\",\n",
        "\"thats\" : \"that is\",\n",
        "\"there's\" : \"there is\",\n",
        "\"theres\" : \"there is\",\n",
        "\"they'd\" : \"they would\",\n",
        "\"they'll\" : \"they will\",\n",
        "\"they're\" : \"they are\",\n",
        "\"theyre\":  \"they are\",\n",
        "\"they've\" : \"they have\",\n",
        "\"we'd\" : \"we would\",\n",
        "\"we're\" : \"we are\",\n",
        "\"weren't\" : \"were not\",\n",
        "\"we've\" : \"we have\",\n",
        "\"what'll\" : \"what will\",\n",
        "\"what're\" : \"what are\",\n",
        "\"what's\" : \"what is\",\n",
        "\"what've\" : \"what have\",\n",
        "\"where's\" : \"where is\",\n",
        "\"who'd\" : \"who would\",\n",
        "\"who'll\" : \"who will\",\n",
        "\"who're\" : \"who are\",\n",
        "\"who's\" : \"who is\",\n",
        "\"who've\" : \"who have\",\n",
        "\"won't\" : \"will not\",\n",
        "\"wouldn't\" : \"would not\",\n",
        "\"you'd\" : \"you would\",\n",
        "\"you'll\" : \"you will\",\n",
        "\"you're\" : \"you are\",\n",
        "\"you've\" : \"you have\",\n",
        "\"'re\": \" are\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'll\":\" will\",\n",
        "\"didn't\": \"did not\",\n",
        "\"tryin'\":\"trying\"}\n",
        "\n",
        "\n",
        "def clean_text(x):\n",
        "    x = str(x).replace(\"\\n\",\"\")\n",
        "    for punct in puncts:\n",
        "        x = x.replace(punct, f' {punct} ')\n",
        "    return x\n",
        "\n",
        "\n",
        "def clean_numbers(x):\n",
        "    x = re.sub('[0-9]{5,}', '#####', x)\n",
        "    x = re.sub('[0-9]{4}', '####', x)\n",
        "    x = re.sub('[0-9]{3}', '###', x)\n",
        "    x = re.sub('[0-9]{2}', '##', x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTv7LSfgc0HM",
        "colab_type": "text"
      },
      "source": [
        "Избавляемся от типичных ошибок, сокращений и тд"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yhzU4oFpL1Et",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "def handle_contractions(x):\n",
        "    x = tokenizer.tokenize(x)\n",
        "    return x\n",
        "\n",
        "def fix_quote(x):\n",
        "    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n",
        "    x = ' '.join(x)\n",
        "    return x\n",
        "\n",
        "def _get_mispell(mispell_dict):\n",
        "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
        "    return mispell_dict, mispell_re\n",
        "\n",
        "\n",
        "def replace_typical_misspell(text):\n",
        "    mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
        "\n",
        "    def replace(match):\n",
        "        return mispellings[match.group(0)]\n",
        "\n",
        "    return mispellings_re.sub(replace, text)\n",
        "\n",
        "\n",
        "def clean_data(df, columns: list):\n",
        "    for col in columns:\n",
        "\n",
        "        df[col] = df[col].apply(lambda x: clean_text(x.lower())) \n",
        "        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n",
        "        df[col] = df[col].apply(lambda x: handle_contractions(x))  \n",
        "        df[col] = df[col].apply(lambda x: fix_quote(x))   \n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "woP_CJ6OL1Ey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_columns = [\n",
        "    'comment_text'   \n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "train = clean_data(train, input_columns ) \n",
        "val = clean_data(val, input_columns )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FDLHPsSxL1FC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_columns = [\n",
        "    'content'   \n",
        "]\n",
        "test_data = clean_data(test_data, input_columns )\n",
        "\n",
        "del tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o-uM6HsdGb7",
        "colab_type": "text"
      },
      "source": [
        "И только теперь подаем очищенный датасет в токенайзер модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "23N_kb0cL1FG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# сам токенайзер\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DUnmocEZL1FL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = regular_encode(train.comment_text.astype(str), \n",
        "                      tokenizer, maxlen=MAX_LEN)\n",
        "x_valid = regular_encode(val.comment_text.astype(str).values, \n",
        "                      tokenizer, maxlen=MAX_LEN)\n",
        "x_test = regular_encode(test_data.content.astype(str).values, \n",
        "                     tokenizer, maxlen=MAX_LEN)\n",
        "\n",
        "y_valid = val.toxic.values\n",
        "y_train = train.toxic.values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVHow2-gdMir",
        "colab_type": "text"
      },
      "source": [
        "Создаем тф датасет"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QePd1IIiL1FS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_train, y_train))\n",
        "    .repeat()\n",
        "    .shuffle(2048)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "valid_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_valid, y_valid))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "test_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices(x_test)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwW0cjVPeR3U",
        "colab_type": "text"
      },
      "source": [
        "Создаем кастомный лосс (focal loss, который хорошо работает для несбалансированной классификации)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpRvZCkjfVne",
        "colab_type": "text"
      },
      "source": [
        "Создаем модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7XM5sSCWL1Fo",
        "colab_type": "code",
        "outputId": "1e34ed4e-7d4c-4462-fb1a-6caeb20ddca7",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "with strategy.scope():\n",
        "    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
        "    model = build_model(transformer_layer, max_len=MAX_LEN, lr=3e-5)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_word_ids (InputLayer)  [(None, 192)]             0         \n",
            "_________________________________________________________________\n",
            "tf_roberta_model (TFRobertaM ((None, 192, 1024), (None 559890432 \n",
            "_________________________________________________________________\n",
            "tf_op_layer_strided_slice (T [(None, 1024)]            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 1025      \n",
            "=================================================================\n",
            "Total params: 559,891,457\n",
            "Trainable params: 559,891,457\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "CPU times: user 28.4 s, sys: 26.7 s, total: 55.1 s\n",
            "Wall time: 59.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0YSAqyT3JZK",
        "colab_type": "text"
      },
      "source": [
        "Также используем focal loss, которая хорошо подходит для несбалансированных выборок - как в данном случае. Создаем лернинг рэйт шэдулер"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPsr5Zae3FCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def focal_loss(gamma=2., alpha=.2):\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
        "    return focal_loss_fixed\n",
        "\n",
        "def build_lrfn(lr_start=0.000001, lr_max=0.000002, \n",
        "               lr_min=0.0000001, lr_rampup_epochs=7, \n",
        "               lr_sustain_epochs=0, lr_exp_decay=.87):\n",
        "    lr_max = lr_max * strategy.num_replicas_in_sync\n",
        "\n",
        "    def lrfn(epoch):\n",
        "        if epoch < lr_rampup_epochs:\n",
        "            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
        "        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n",
        "            lr = lr_max\n",
        "        else:\n",
        "            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n",
        "        return lr\n",
        "    \n",
        "    return lrfn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "q_XreClTL1Fq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lrfn = build_lrfn()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJwuqscZ31W9",
        "colab_type": "text"
      },
      "source": [
        "Создаем колбэк для обучения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pO9eSzD0L1Fv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "lr_callback = LearningRateScheduler(lrfn, verbose=1)\n",
        "callback_list = [lr_callback]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0T2ne2Uhgn2",
        "colab_type": "text"
      },
      "source": [
        "Теперь также стандартный процесс обучения - данные строчки закомментированы, так как при попытке запустить все модели Roberta во время одного коммита, чтобы сохранить выходы ячеек, все слетает - Exhausted Resourses.  В общем-то тут стандартный процесс обучения - прикрепляю скрины."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pYVHvtbYL1F5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# N_STEPS = x_train.shape[0] // BATCH_SIZE\n",
        "# EPOCHS = 2\n",
        "# train_history = model.fit(\n",
        "#     train_dataset,\n",
        "#     steps_per_epoch=N_STEPS,\n",
        "#     validation_data=valid_dataset,\n",
        "#     callbacks=callback_list,\n",
        "#     epochs=EPOCHS\n",
        "# )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "d7dwkNmiL1F9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EPOCHS = 4\n",
        "# n_steps = x_valid.shape[0] // BATCH_SIZE\n",
        "# train_history_2 = model.fit(\n",
        "#     valid_dataset.repeat(),\n",
        "#     steps_per_epoch=n_steps,\n",
        "#     callbacks=callback_list,\n",
        "#     epochs= EPOCHS\n",
        "# )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxJqwwdr-__R",
        "colab_type": "text"
      },
      "source": [
        "Процесс обучения на трейн:\n",
        "\n",
        "![alt text](https://sun9-8.userapi.com/c857528/v857528479/205d89/VTZV4f0NJsE.jpg)\n",
        "\n",
        "Обучение на валидационной части:\n",
        "\n",
        "![alt text](https://sun9-61.userapi.com/c857528/v857528479/205d91/fj_Gb8vCcEk.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4ilNfoliLje",
        "colab_type": "text"
      },
      "source": [
        "Сохранение предсказания"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ih99_BafL1GB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sub['toxic'] = model.predict(test_dataset, verbose=1)\n",
        "# sub.to_csv('submission_5.csv', index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2GEWUyBiQDE",
        "colab_type": "text"
      },
      "source": [
        "Теперь, когда уже все три предсказания сохранены как аутпуты ноутбука - подгружаем их в папку инпут, считываем, объединяем, умножая на коэффициенты и получаем итоговый сабмит."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7FjvQ_hRL1GG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub1 = pd.read_csv(\"/kaggle/input/submission-roberta4/submission_4.csv\")\n",
        "sub2 = pd.read_csv(\"/kaggle/input/submission-roberta5/submission_5.csv\")\n",
        "sub3 = pd.read_csv(\"/kaggle/input/submissionroberta6/submission_6.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SxVXssw3L1GO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub1['toxic'] = sub1['toxic']*0.3 + sub2['toxic']*0.5 + sub3['toxic']*0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv3M4yF3ikWC",
        "colab_type": "text"
      },
      "source": [
        "Сохраняем его и посылаем в кэггл"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aPxSc-9bL1GS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub1.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dky6QQI8tgD7",
        "colab_type": "text"
      },
      "source": [
        "Финальный результат:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yStgs-vqrZnz",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://sun9-11.userapi.com/c854120/v854120403/236734/N61N0rJ4Bxs.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faMH1hjLEERU",
        "colab_type": "text"
      },
      "source": [
        "Таким образом, объединение предсказаний нескольких моделей позволяет несколько улучшить качество"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO0Ffubawjku",
        "colab_type": "text"
      },
      "source": [
        "references:\n",
        "\n",
        "опять же почти все ноутбуки с кэггл по соревнованию и еще отдельно стоит отметить вот этот чудесный кернел, откуда были взяты функция для предобработки данных и любопытная focal loss: https://www.kaggle.com/mobassir/understanding-cross-lingual-models"
      ]
    }
  ]
}